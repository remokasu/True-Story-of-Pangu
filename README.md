# 盘古之殇：ファーウェイ 大規模言語モデル研究開発の辛酸と暗黒

皆さん、こんにちは。

私は、ファーウェイのノアズアーク研究所の従業員であり、盤古(Pangu)大規模言語モデルチームに所属しています。。

まず身元証明のため、いくつかの詳細を列挙する。

1. 現ノア所長、元アルゴリズム応用部部長、後に小規模モデル実験室に改名された所長の王雲鶴。前ノア所長：姚駿（皆が姚先生と呼ぶ）。各実験室所長：唐睿明（明哥、明隊、既に退職）、尚利峰、張維（維哥）、郝建業（郝先生）、劉武龍（武龍所と呼ばれる）など。その他の中核メンバーや専門家も続々と退職している。

2. 我々は「四野」という組織に所属している。四野の下には多くの部隊があり、基盤言語モデルチームは四縦。王雲鶴の小規模モデルチームは十六縦隊。我々は蘇州での合宿に参加し、様々な月次マイルストーンがある。蘇州攻関会でミッション指令を発布し、マイルストーン前に目標達成が必要。蘇州合宿は各地の人員を蘇州研究所に集中させ、普段はホテル住まい、例えば甪直（ろっちょく）のホテルで、家族や子供とは離ればなれ。

3. 蘇州集結時は土曜日もデフォルトで出勤、非常に辛い。ただし土曜日にはアフタヌーンティーがあり、一度は小龍蝦もあった。蘇州研究所の席は一度移転し、一つの棟から別の棟へ。蘇州研究所の建物はすべてヨーロッパ風装飾で、入口に大きな坂があり、内部の景色は素晴らしい。蘇州集結は通常最低一週間、さらに長期間で、多い人は一、二ヶ月も家に帰れない。

4. ノアはかつて研究型と言われていたが、来てみると四野で大規模言語モデルプロジェクトをやるため、プロジェクトメンバーは完全に納品型になり、会議、審査、報告に満ちている。実験をするのも申請が必要なことが多い。チームは端末Xiaoyi（小芸）、ファーウェイクラウド、ICTなど多数の事業部と連携する必要があり、納品プレッシャーは小さくない。

5. ノア研究開発の盤古モデルの初期内部コードネームは「盤古智子」と呼ばれ、最初は内部申請試用のウェブ版のみで、後にプレッシャーによりWeLinkで統合し公開テストを開始。

この数日、盤古大規模言語モデルがQwen（千問）を盗用したという疑惑で騒然としている。盤古チームのメンバーとして、私は最近夜な夜な寝返りを打ち、眠れない日々を送っている。盤古のブランドがこれほど大きな影響を受け、一方では、私は利己的に自分のキャリア発展を心配し、過去に努力した仕事が報われないと感じている。もう一方では、誰かがこれらのことを暴露し始めたため、内心では大いに溜飲を下げている。どれほど多くの日夜、我々は内部の一部の人々が繰り返し偽造によって無数の利益を得る行為に歯噛みしながらも無力であった。この抑圧と屈辱は徐々に私のファーウェイへの感情をすり減らし、ここでの日々を次第に漫然と過ごし、迷い戸惑い、しばしば自分の人生と自己価値を疑うようになった。

私は自分が臆病者であることを認める。一介の労働者として、私は王雲鶴など内部で手腕を振るう人々と対立することを恐れるだけでなく、ファーウェイのような巨大企業と対立することをさらに恐れている。私は職を失うことをとても恐れている。結局、私にも家族と子供がいるからだ。だから私は心から内部告発者を尊敬している。しかし、内部がまだ事実を隠蔽し公衆を欺こうとしているのを見ると、私は本当に我慢できなくなった。私も一度勇敢になり、自分の本心に従いたい。刺し違えてでも一矢報いたい。私は自分がここで見聞きしたこと（一部は同僚からの証言）を公表することを決めた。盤古大規模言語モデルの「伝説的物語」について：

ファーウェイは確かに主にAscend（昇騰）チップで大規模言語モデルを学習している（小規模モデル実験室にはNVIDIAのカードもかなりあり、以前は学習に使っていたが、後にAscendに移行）。かつて私はファーウェイの「世界第二の選択肢を作る」という決意に感服し、私自身もかつてファーウェイに深い愛着を抱いていた。我々はAscendと一歩一歩這い上がり、バグだらけの状態から現在モデルを学習できるまで、巨大な労力と犠牲を払った。

最初我々の計算資源は非常に限られており、910Aでモデルを学習していた。その時はfp16のみサポートで、学習の安定性はbf16には遠く及ばなかった。盤古のMoEは非常に早く始まり、23年には主に380億パラメータMoEモデルと後続の710億パラメータDense型モデルを学習していた。710億パラメータDense型モデルはパラメータ拡張により第一世代の1350億パラメータDense型モデルになり、後の主力モデルも徐々に910Bで学習するようになった。

710億と1350億パラメータモデルは、どちらもトークナイザーに深刻な問題を抱えていた。当時使用していたトークナイザーのエンコード効率は極めて低く、単一の記号、数字、空白、さらには漢字まで一つのトークンを占有していた。これが非常に計算資源を浪費し、モデルの性能を非常に悪くすることは想像に難くない。この時、小規模モデル実験室がちょうど自分で学習した語彙辞書を持っていた。姚先生は当時モデルのトークナイザーが良くないのではないかと疑い（事後から見ると、彼の疑いは間違いなく正しかった）、そこで710億と1350億のトークナイザーを変更すべきだと決定した、小規模モデル実験室が試したことがあるからと。チームは二つのトークナイザーを継ぎ接ぎし、トークナイザーの交換を開始した。710億パラメータモデルの交換は失敗したが、1350億は より精細な埋め込み層初期化手法を採用し、最低1兆トークンのデータで継続事前学習した後、語彙辞書の交換はようやく成功したが、想像に難くない通り、性能は良くならなかった。

この同時期、アリババやZhiPu（智谱）など国内他社はGPUで学習し、既に正しい方法を模索していたため、盤古と競合他社の差はますます大きくなった。内部の2300億パラメータをフルスクラッチで学習するDense型モデルも様々な理由で学習に失敗し、プロジェクトの状況はほぼ絶境に陥った。いくつかのマイルストーンのプレッシャーと内部の盤古への強烈な疑問に直面した時、チームの士気は極限まで低迷した。チームは計算資源が極めて限られた中で、多くの努力と奮闘をした。例えば、チームは偶然当時の380億パラメータMoEが期待していたMoEの効果を得ていないことを発見した。そこでMoEパラメータを除去し、130億パラメータのDense型モデルに復元した。380億パラメータのMoEは極めて初期のPangu Alpha 130億に由来するため、アーキテクチャが相対的に遅れており、チームは一連の改良を行った。例えば絶対位置エンコーディングからRoPEへの切り替え、バイアスの除去、RMSNormへの切り替え。同時にトークナイザーのいくつかの失敗と語彙辞書変更の経験を踏まえ、このモデルの語彙辞書も王雲鶴の小規模モデル実験室70億パラメータモデルが使用していた語彙辞書に交換した。後にこの130億パラメータモデルはスケールアップして継続事前学習され、第二世代380億パラメータDense型モデル（数ヶ月間このモデルは主要な盤古ミドルレンジモデルであった）になり、かつて一定の競争力を持っていた。しかし、より大きな1350億パラメータモデルはアーキテクチャが遅れており、語彙辞書変更でモデルが大きく損傷し（後の分析で当時変更した継ぎ接ぎ語彙辞書にはより深刻なバグがあることが判明）、継続事前学習後もQwen（千問）など当時国内先進モデルと大きな差があった。この時、内部の疑問の声と指導者のプレッシャーもますます大きくなった。チームの状態はほぼ絶境に陥った。

この状況下で、王雲鶴と彼の小規模モデル実験室が乗り出した。彼らは旧1350億パラメータモデルから継承改造したと表明し、短い数百億トークンのデータを学習しただけで、各項目指標が平均10ポイント程度向上したとした。実際、これは彼らが他社の大規模言語モデルに薄皮を被せて応用した第一回の傑作だった。ファーウェイは素人が玄人を指導する体制のため、指導者はこのような馬鹿げたことに全く理解がなく、何らかのアルゴリズム革新があったに違いないと思うだけだった。内部分析を経て、彼らは実際にはQwen 1.5の1100億パラメータモデルを継続事前学習したもので、層の追加、FFN次元の拡張、盤古π論文の一部メカニズム追加により、約1350億パラメータを揃えたものだった。実際、旧1350億パラメータモデルは107層だったが、このモデルは82層のみで、各種設定も全て異なっていた。新しい出所不明の1350億パラメータモデルは学習完了後、多くのパラメータ分布もQwen 1100億パラメータモデルとほぼ同一だった。モデルコードのクラス名は当時すべてQwenで、名前を変更するのも面倒がった。後にこのモデルは所謂1350億パラメータ V2となった。そしてこのモデルは当時多くの下流タスク、さらには外部顧客にも提供された。

この事件は我々のような真面目で誠実に仕事をする同僚たちに巨大な衝撃をもたらした。内部の多くの人、さらには端末事業部やファーウェイクラウド事業部を含めて実はこの事を知っていた。我々は皆冗談で今後盤古モデルと呼ぶのは止めて、千古（Qwen＋Pangu）と呼ぼうと言った。当時チームメンバーはBCG（企業倫理違反通報制度）に通報しようと思った。結局これは既に重大な業務偽造だったからだ。しかし後に上層部に止められたと言われている。より上級レベルの指導者（例えば姚先生、そして恐らく熊総と査老）も実は後に知ったが、問題視しなかった。他社モデルに薄皮を被せて良い結果を出すことは、彼らにとっても都合が良かったからだ。この事により当時チームの最も優秀な数名の同僚が意気消沈し、退職して逃げ出すことも徐々に口癖になった。

この時、盤古は転機を迎えたようだった。前述のこれらの盤古モデルは基本的に継続事前学習と改造から来ているため、当時ノアは完全にフルスクラッチ学習の技術を掌握しておらず、況してやAscend NPUで学習することは言うまでもなかった。当時チームの中核メンバーが必死に働きかけた結果、盤古は第三世代モデルの学習を開始し、巨大な努力を払った後、データ、アーキテクチャ、学習アルゴリズムの面で業界標準と徐々に歩調を合わせることができた。その中の苦労は小規模モデル実験室の人々とは全く無関係だった。

最初チームメンバーは全く自信がなく、130億パラメータモデルから学習を開始したのみだったが、後に性能がまずまずであることを発見し、そこでこのモデルは後に再度パラメータ拡張を行い、第三世代の380億パラメータ、コードネーム38B V3になった。恐らく多くの製品部門の同僚たちはこのモデルに馴染みがあるだろう。当時このモデルのトークナイザーはLLaMAの語彙辞書を基礎として拡張したもの（業界の一般的な手法でもある）だった。そして当時王雲鶴の実験室は別の語彙辞書を作り出した（すなわち後続Panguシリーズの語彙辞書）。当時二つの語彙辞書は強制的に一度比較テストを行い、最終的に明確な優劣の結論はなかった。そこで、指導者は即座に決定し、語彙辞書を統一すべきで、王雲鶴らのものを使用すべきだとした。そこで、後続フルスクラッチ学習の1350億パラメータ V3（すなわち対外的なPangu Ultra）では、このトークナイザーを採用した。これは我々のモデルを使用する多くの同僚の疑問を説明している。なぜ当時同じV3世代の二つの異なるクラスのモデルが、異なるトークナイザーを使用するのか。

我々は心から、1350億パラメータ V3は我々四縦チームの当時の誇りだと思っている。これは真の意味で初めての、ファーウェイ フルスタック自主研究、正当なフルスクラッチ学習の千億級レベルのモデルで、性能は24年同期の競合他社と比較可能だった。ここまで書いて私は既に涙が溢れている。本当に容易ではなかった。当時安定学習のため、チームは大量の実験比較を行い、モデル勾配に異常が現れた時に何度も適時にロールバックして再起動を行った。このモデルは真に後の技術報告で述べられた学習全過程で損失の急激な増加が一度もなかったことを実現した。我々はどれほど多くの困難を克服したか分からない。我々はやり遂げた。我々は生命と名誉をかけてこのモデル学習の真実性を保証する。どれほど多くの夜明け、我々はその学習のために眠らなかった。内部の匿名掲示板で一銭の価値もないと罵られた時、我々はどれほど悔しく、どれほどの屈辱があったか、我々は耐え抜いた。

我々一同は真に国産計算資源基盤を磨き上げるために自分の青春を燃やしているのだ...故郷を離れ、我々は家庭を犠牲にし、休日を犠牲にし、健康を犠牲にし、娯楽を犠牲にし、身を粉にして働き、その中の艱難辛苦は、わずかな筆致では万分の一も表現し尽くせない。様々な動員大会で、当時スローガンの中で叫ばれた「盤古必勝、華為必勝」という言葉に、我々の心は本当に深く感動したのだ。

しかし、我々のすべての苦労の成果は、しばしば小規模モデル実験室に軽々と奪われた。データは、直接奪取。コードは、直接奪取し、さらに我々にワンクリックで実行できるよう適応と協力を要求した。我々は当時小規模モデル実験室をマウスクリック実験室と冗談で呼んだ。我々が苦労を重ね、彼らが栄光を得る。まさにあの言葉の通り、あなたが重荷を背負って前進するのは、誰かがあなたの代わりに平穏な日々を過ごしているからだ。この状況下で、ますます多くの戦友がもはや耐えられず、退職を選択した。周りの優秀な同僚が一人また一人と退職するのを見て、私の内心は感嘆と悲しみに満ちた。この戦場のような環境下で、我々は同僚というより戦友に近い。彼らは技術面でも私が学ぶべき無数の価値があり、良師と呼べる。彼らがByteDance Seed、DeepSeek、月之暗面（Kimi）、Tencent、Kuaishouなど多くの優秀なチームに行ったのを見て、私は心から彼らのために喜び祝福している。この辛いが汚れた場所から脱出したのだ。私は今でも一人の退職同僚の言葉を鮮明に覚えている。「ここに来たのは私の技術キャリアの恥辱で、ここでさらに一日いることは人生の浪費だ」。言葉は辛辣だが、私は反論できなかった。私は自分の技術面での蓄積不足と、IT企業の激しい競争環境に適応できるか心配で、何度も退職したいと思いながらも、この一歩を踏み出せずにいる。

盤古はDense型モデル以外に、後にMoEの探索も開始した。最初に学習したのは2240億パラメータのMoEモデルだった。それと並行して、小規模モデル実験室も第二回目の主要な他社モデル偽装行動を開始した（些細な挿話には恐らく他のモデル、例えば数学モデルなども含まれる）、すなわち今回広く流布したPangu Pro MoE 720億パラメータ。このモデルは内部で小規模モデル実験室の70億パラメータから拡張したと自称していた（そうだとしても、これも技術報告に合致せず、況してやQwen 2.5の140億パラメータを継続事前学習した偽装モデルである）。彼らが数日学習しただけで、内部評価がすぐに当時の380億パラメータ V3に追いついたことを覚えている。AIシステム実験室の多くの同僚はモデル適応が必要なため、皆彼らの偽装行動を知っているが、様々な理由で正義を貫徹できない。実際、後続非常に長期間学習したこのモデルについて、HonestAGIがこのレベルの類似性を分析できたことは私には既に驚きだった。なぜならこのモデルは継続事前学習でパラメータを洗浄するため、費やした計算資源は早くにフルスクラッチで同クラスのモデルを学習するのに十分だったからだ。同僚によると、彼らはQwenの電子透かしを除去するため、少なからぬ手法を採用し、さらには故意にノイズデータを学習したという。これも学術界のモデル系譜研究に前例のない特殊な事例を提供しただろう。今後新しい系譜追跡手法が提案されたら検証に使うことができる。

24年末と25年初、DeepSeek V3とR1発表後、その驚異的な技術水準により、チームは巨大な衝撃を受け、より大きな疑問も受けた。そこで潮流に追随するため、盤古はDeepSeekのモデルサイズを模倣し、7180億パラメータMoEの学習を開始した。この時、小規模モデル実験室が再度乗り出した。彼らはDeepSeek V3の偽装継続事前学習を選択した。彼らはDeepSeek読み込みパラメータを凍結して学習を行った。タスク読み込みチェックポイントのディレクトリまでdeepseekv3で、変更すらしない。何と傲慢な！それとは対照的に、一部の真の技術への信念を持つ同僚は、フルスクラッチで別の7180億パラメータMoEを学習していた。しかしその中で様々な問題が現れた。しかし明らかに、このモデルが直接偽装したものより良いはずがない。もしチームリーダーが断固として主張しなければ、とっくに中止されていただろう。

ファーウェイのプロセス管理の煩雑さは、大規模言語モデルの研究開発ペースを深刻に遅らせた。例えばバージョン管理、モデル系譜、各種プロセス化、各種トレーサビリティ。皮肉なことに、小規模モデル実験室のモデルはこれらのプロセスの制約を受けることがないようで、偽装したければ偽装、継続事前学習したければ継続事前学習、計算資源も絶えず手を伸ばして奪取する。この強烈で幻想的とも言える対比は、現在のプロセス管理の状況を説明している。「お上の放火は許すが、庶民のろうそくは許さない」。何と滑稽な！何と悲しい！何と憎らしい！何と恥ずべきことか！

HonestAGIの件が出た後、内部は皆に絶えず検討・分析させ、どうPRし「対応」するかを考えさせた。確かに、この原文の分析は恐らく十分説得力がなく、王雲鶴と小規模モデル実験室に詭弁と事実歪曲の機会を与えた。このため、この二日間私の内心は嫌悪感を覚え、時々自分の人生の意義と天の不公正を疑っている。私はもう付き合いきれない。私は退職する。同時に私は盤古の一部技術報告の著者リストからの除名を申請している。かつてこれらの技術報告に署名したことは私の一生消えない汚点だ。当時私は思わなかった。彼らがこれほど厚かましくオープンソース化を敢行するとは。私は思わなかった。彼らがこれほど世間を愚弄し、大々的に宣伝・発信を敢行するとは。当時、私は恐らく甘い期待を抱き、署名を拒否しなかった。多くの誠実に仕事をする戦友も、ただ強制的に泥舟に乗せられたか、事情を知らなかっただけだと信じている。しかしこの事は既に取り返しがつかない。私は残りの人生で誠実に真に意義のある事を貫き、当時の弱さと意志の弱さを償いたい。

深夜ここまで書いて、私は既に涙が流れ、嗚咽で声も出ない。まだ一部の優秀な同僚が退職する時、私が苦笑いしながら彼らにお決まりの長い退職投稿を社内掲示板に出して現状を暴露しないかと尋ねたことを覚えている。相手は言った。「やらない、時間の無駄だし、暴露したらあなたたちがより辛い思いをするのも怖い。」私は当時一瞬暗澹とし、心を痛めた。かつて共に理想のために奮闘した戦友が既にファーウェイに完全に失望していたからだ。当時皆が冗談を言った。我々は昔の共産党のお粗末な装備を使っているが、組織は昔の国民党に匹敵する体質を持っている。

かつて私は、我々がお粗末な装備で米帝の最新兵器を打ち負かすことを誇りに思った。

今、私は疲れた、私は降伏したい。

実は今日に至っても、私は心からファーウェイが真剣に教訓を汲み取り、盤古をしっかりと開発し、盤古を世界一流にし、Ascendを NVIDIAのレベルにできることを希望している。内部で悪貨が良貨を駆逐する状況により、ノアひいてはファーウェイは短期間で急激に大量の優秀な大規模言語モデル人材を失った。彼らはDeepSeekなど各チームで輝き、その志と才能を発揮し、米中AI激烈競争で力を貢献していると信じている。私はしばしば感嘆する。ファーウェイは人材がいないのではなく、人材をどう引き留めるかを全く知らないのだ。もしこれらの人に適切な環境、適切な資源、より少ない束縛、より少ない政治的争いを与えたら、盤古の将来を憂う必要があるだろうか？

最後に：私は生命、人格と名誉をかけて誓う。私が書いた以上のすべての内容は真実である（少なくとも私の限られた認知範囲内で）。私にはそれほど高い技術水準と機会がなく詳細で堅実な分析を行い、内部記録で直接証拠を挙げることも敢えてしない。情報セキュリティ違反で捕まるのが怖いからだ。しかし私の多くのかつての戦友が私のために証言してくれると信じている。ファーウェイ内部の同僚、我々がかつてサービスした製品部門の同僚たちを含め、本文の無数の細節があなたたちの記憶と照合でき、私の主張を証明できると信じている。あなたたちも恐らくかつて騙されたかもしれないが、これらの残酷な真相が封印されることはない。我々が奮戦した痕跡も、歪曲され埋葬されるべきではない。

これほど多く書いたが、一部の人々は必ず私を見つけ出し、口封じしたがるだろう。会社も恐らく私を沈黙させ、さらには責任追及したがるだろう。もし本当にそうなら、私、ひいては私の家族の身の安全、さらには生命の安全も脅威を受ける可能性がある。自己保護のため、私は近々毎日皆に安否報告をする。

もし私が姿を消したら、私が真理と理想のため、ファーウェイひいては中国の計算資源とAIのより良い発展のために犠牲になったと思ってほしい。私はかつて奮闘したあの土地に埋葬されたい。

ノア、さらば

2025年7月6日未明　深圳にて
